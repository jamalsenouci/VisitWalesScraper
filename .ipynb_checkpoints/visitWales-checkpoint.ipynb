{
 "metadata": {
  "name": "",
  "signature": "sha256:8980497476c87b30e755af8f8074da9786ee5922253e940eb6a097a8fdcd6a71"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Create new repo using github api"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "access_token = input()\n",
      "url = \"https://api.github.com/user/repos?access_token=\"+access_token\n",
      "data = '{\"name\":\"snowdonia-scrape\", \"description\":\"a scrapy crawler to get activities in Snowdonia listed on visitwales.com\"}'\n",
      "requests.post(url, data)\n",
      "\n",
      "mkdir snowdonia-scrape\n",
      "cd snowdonia-scrape/\n",
      "!git init\n",
      "!git remote add origin git@github.com:jamalsenouci/snowdonia-scrape.git"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Scrapy project setup"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!scrapy startproject visitWales\n",
      "!ls -R visitWales"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!git add .\n",
      "!git commit -m \"Project setup\"\n",
      "!git push origin master"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Create activity class to contain scraped data with all the field of interest."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile visitWales/visitWales/items.py\n",
      "\n",
      "from scrapy.item import Item, Field\n",
      "\n",
      "class Activity(Item):\n",
      "    name = Field()\n",
      "    addr = Field()\n",
      "    site = Field()\n",
      "    tel = Field()\n",
      "    desc = Field()\n",
      "    catid = Field()\n",
      "    catname = Field()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Main Crawler"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Crawls through all the activity results by category and follows links to get detailed information"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile visitWales/visitWales/spiders/pages_spider.py\n",
      "\n",
      "import scrapy\n",
      "from scrapy.contrib.spiders import CrawlSpider, Rule\n",
      "from scrapy.contrib.linkextractors import LinkExtractor\n",
      "import re\n",
      "from visitWales.items import Activity\n",
      "\n",
      "base = \"http://www.visitwales.com/activity-search/activity-search-results?\\\n",
      "location=Snowdonia+Mountains+and+Coast&industry=Activities&radius=10&filterIds=\"\n",
      "\n",
      "#each category has one of the filter ids below\n",
      "filters = [ 'Index-101321143398%252c',\n",
      "            'Index-101321143728%252c',\n",
      "            'Index-101321143513%252c',\n",
      "            'Index-101321143401%252c',\n",
      "            'Index-101321143771%252c',\n",
      "            'Index-101321143392%252c',\n",
      "            'Index-1013211434034209%252c',\n",
      "            'Index-101321143402%252c',\n",
      "            'Index-101321143411%252c',\n",
      "            'Index-101520163302%252c',\n",
      "            'Index-101321173346%252c',\n",
      "            'Index-101520093023%252c',\n",
      "            'Index-101322143514%252c',\n",
      "            'Index-101322143512%252c',\n",
      "            'Index-101322143415%252c',\n",
      "            'Index-101322143417%252c',\n",
      "            'Index-101322143518%252c',\n",
      "            'Index-101322143418%252c',\n",
      "            'Index-101322143421%252c',\n",
      "            'Index-101322143420']\n",
      "\n",
      "sites = [base+filt for filt in filters]\n",
      "\n",
      "#the spider will crawl through each of the category pages and follow links to the activities, parsing only the activity pages\n",
      "class pagesSpider(CrawlSpider):\n",
      "    name = \"pages\"\n",
      "    download_delay = 2\n",
      "    allowed_domains = [\"visitwales.com\"]\n",
      "    start_urls = sites\n",
      "    rules = (Rule (LinkExtractor(restrict_xpaths=('//*[@id=\"next\"]',))),\n",
      "    Rule (LinkExtractor(restrict_xpaths=('//*[@id=\"mainform\"]/div[3]/div/div[1]/article/div/section/ul[2]/li/a[1]',))\n",
      "    , callback=\"parse_items\"),\n",
      "    )\n",
      "    \n",
      "    def parse_items(self, response):\n",
      "        item = Activity()\n",
      "        s = response.url\n",
      "        start = 'filterIds='\n",
      "        end = '&'\n",
      "        item['catid'] = [re.search('%s(.*?)%s' % (start, end), s).group(1)]\n",
      "        item['name'] = response.xpath('//*[@id=\"mainform\"]/div[3]/div/div[1]/section/h1/text()').extract()\n",
      "        item['addr'] = response.xpath('//*[@id=\"mainform\"]/div[3]/div/div[1]/section/h3/text()').extract()\n",
      "        item['tel'] = response.xpath('//*[@id=\"mainform\"]/div[3]/div/div[1]/section/ul/li[1]/span/text()').extract()\n",
      "        item['site'] = response.xpath('//*[@id=\"main_0_content_0_WebLink\"]/@href').extract()\n",
      "        item['desc'] = response.xpath('//*[@id=\"mainform\"]/div[3]/div/div[1]/article/div/section/div[1]/p/text()').extract()\n",
      "        yield item"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Crawler to match up category indices with category names"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile visitWales/visitWales/spiders/cat_spider.py\n",
      "\n",
      "import scrapy\n",
      "from visitWales.items import Activity\n",
      "\n",
      "base = \"http://www.visitwales.com/activity-search/activity-search-results?\\\n",
      "location=Snowdonia+Mountains+and+Coast&industry=Activities&radius=10&filterIds=\"\n",
      "\n",
      "filters = [ 'Index-101321143398%252c',\n",
      "            'Index-101321143728%252c',\n",
      "            'Index-101321143513%252c',\n",
      "            'Index-101321143401%252c',\n",
      "            'Index-101321143771%252c',\n",
      "            'Index-101321143392%252c',\n",
      "            'Index-1013211434034209%252c',\n",
      "            'Index-101321143402%252c',\n",
      "            'Index-101321143411%252c',\n",
      "            'Index-101520163302%252c',\n",
      "            'Index-101321173346%252c',\n",
      "            'Index-101520093023%252c',\n",
      "            'Index-101322143514%252c',\n",
      "            'Index-101322143512%252c',\n",
      "            'Index-101322143415%252c',\n",
      "            'Index-101322143417%252c',\n",
      "            'Index-101322143518%252c',\n",
      "            'Index-101322143418%252c',\n",
      "            'Index-101322143421%252c',\n",
      "            'Index-101322143420']\n",
      "\n",
      "sites = [base+filt for filt in filters]\n",
      "\n",
      "\"\"\"\n",
      "this spider loops through the category pages, checking which category checkbox is checked \n",
      "and extracts the label text to match to the index\n",
      "\"\"\"\n",
      "class catSpider(scrapy.Spider):\n",
      "    name = \"cat\"\n",
      "    download_delay = 2\n",
      "    allowed_domains = [\"visitwales.com\"]\n",
      "    start_urls = sites\n",
      "    \n",
      "    def parse(self, response):\n",
      "        item = Activity()\n",
      "        s = response.url\n",
      "        item['catid'] = s.split('filterIds=')[1]\n",
      "        item['catname'] = response.xpath(\"//*[@checked='checked']/../label/text()\").extract()\n",
      "        yield item"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Start Crawlers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#run the scrapers and save output as json\n",
      "! cd visitWales/; scrapy crawl pages -o pages.json\n",
      "! cd visitWales/; scrapy crawl cat -o cat.json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Output to json"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "! cd visitWales/; scrapy crawl cat -o cat.json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Format and Clean"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import json\n",
      "import re\n",
      "\n",
      "#load the activities json into a dataframe\n",
      "data = json.load(open('visitWales/pages.json'))\n",
      "activities = pd.DataFrame(data)\n",
      "\n",
      "#load the categories json into a dataframe\n",
      "data = json.load(open('visitWales/cat.json'))\n",
      "categories = pd.DataFrame(data)\n",
      "\n",
      "#formatting for the json\n",
      "def format(x):\n",
      "    if len(x)==0:\n",
      "        x = ''\n",
      "    elif len(x)==1:\n",
      "        x = x[0]\n",
      "    else:\n",
      "        x = \"\".join(x)\n",
      "    regex = re.compile('[\\s{2,}]*\\r\\n[\\s{2,}]*')\n",
      "    return re.sub(regex,'', x)\n",
      "\n",
      "#apply the formatting to each cell\n",
      "activities = activities.applymap(lambda x: format(x))\n",
      "categories = categories.applymap(lambda x: format(x))\n",
      "\n",
      "#some bug adding watersports to every category, fix by removing string\n",
      "categories = categories.applymap(lambda x: x.replace('Water Sports', ''))\n",
      "\n",
      "#create a new category index \n",
      "categories['catId']=categories.index.values\n",
      "\n",
      "#join activities to categories\n",
      "activities = pd.merge(activities,categories)\n",
      "\n",
      "#drop the old id\n",
      "activities.drop('catid',axis=1)\n",
      "\n",
      "#rename columns\n",
      "activities.rename(columns={'catname':'Category', 'addr':'Address','name':'Name','desc':'Description', 'site':'Website','tel':'Telephone'}, inplace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Geocode the addresses using the google geocoding api"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#geocoding function\n",
      "import time\n",
      "def geocode(addr):\n",
      "    addr = addr.replace(' ', '+')\n",
      "    addrlist = addr.split(',')\n",
      "    #try and geocode full address, if fails then geocode approximate location\n",
      "    for i in range(len(addrlist)):\n",
      "        #api limits to 5 requests a second\n",
      "        time.sleep(1/5)\n",
      "        addr = \"\".join(addrlist[i:])\n",
      "        url=\"https://maps.googleapis.com/maps/api/geocode/json?address=%s\" % addr\n",
      "        response = requests.get(url)\n",
      "        jsongeocode = json.loads(response.content())\n",
      "        try:\n",
      "            lat = jsongeocode['results'][0]['geometry']['location']['lat']\n",
      "            lng = jsongeocode['results'][0]['geometry']['location']['lng']\n",
      "            return lat, lng\n",
      "        except:\n",
      "            pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Add latitude and longitude to the output"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lat,lng = zip(*activities['Address'].map(geocode))\n",
      "activities['lat'] = lat \n",
      "activities['lng'] = lng"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Convert to geojson format"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "geojson=[]\n",
      "for x in activities.index:\n",
      "    row = activities.ix[x,:]\n",
      "    feature={}\n",
      "    feature['type']=\"Feature\"\n",
      "    feature['geometry']={}\n",
      "    feature['geometry']['type']=\"Point\"\n",
      "    feature['geometry']['coordinates']= [row['lat'], row['lng']]\n",
      "    feature['properties']={}\n",
      "    feature['properties']['Address']=row['Address']\n",
      "    feature['properties']['Description']=row['Description']\n",
      "    feature['properties']['Name']=row['Name']\n",
      "    feature['properties']['Website']=row['Website']\n",
      "    feature['properties']['Telephone']=row['Telephone']\n",
      "    feature['properties']['Category']=row['Category']\n",
      "    geojson.append(feature)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Export to files"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "activities.to_json('visitWales/activities.json')\n",
      "activities.to_csv('visitWales/activities.csv', encoding='utf-8')\n",
      "with open('geojson.json', 'w') as f:\n",
      "    json.dump(geojson, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Push to github"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!git add .\n",
      "!git commit -m \"Added Crawlers and output to files\"\n",
      "!git push origin master"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}